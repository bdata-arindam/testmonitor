import json
import logging
from confluent_kafka import Consumer, KafkaError
from datetime import datetime
import pandas as pd
import queue

# Load Kafka configuration from the JSON file
with open('kafka_ip_config.json', 'r') as kafka_config_file:
    kafka_config = json.load(kafka_config_file)

# Extract the Kafka topic and consumer tuning properties
kafka_topic = kafka_config.get('kafka_topic', 'default_topic')
consumer_tuning_properties = kafka_config.get('consumer_tuning_properties', {})

# Extract the window size in seconds from Kafka configuration
window_size_seconds = kafka_config.get('window_size_seconds', 3600)

# Create a Kafka consumer instance with tuning properties
consumer = Consumer({**kafka_config, **consumer_tuning_properties})
consumer.subscribe([kafka_topic])

# Initialize a DataFrame with selected columns
data = pd.DataFrame(columns=['timestamp', 'count'])

# Create a queue for data transfer to the anomaly detection module
data_queue = queue.Queue()

# Set up logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('kafka_input.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

def preprocess_message(message):
    try:
        data_dict = json.loads(message.value())

        # Extract columns based on select_columns
        timestamp = data_dict.get('timestamp', None)
        count = data_dict.get('count', None)
        
        return timestamp, count
    except Exception as e:
        logger.error(f"Error processing Kafka message: {str(e)}")
        return None, None

def fetch_data_from_kafka():
    try:
        while True:
            message = consumer.poll(1.0)
            if message is None:
                continue
            if message.error():
                if message.error().code() == KafkaError._PARTITION_EOF:
                    continue
                else:
                    logger.error(f"Kafka error: {message.error()}")
                    break
            else:
                timestamp, count = preprocess_message(message)

                if timestamp is not None:
                    data = data.append({'timestamp': timestamp, 'count': count}, ignore_index=True)

                    # Pass the data to the anomaly detection module via the queue
                    data_queue.put(data)

    except KeyboardInterrupt:
        pass
    finally:
        consumer.close()

if __name__ == "__main__":
    window_size = kafka_config.get('window_size', 3600)

    while True:
        fetch_data_from_kafka()
