import argparse
import json
import threading
import os
import tempfile
import logging
import requests
from confluent_kafka import Consumer, KafkaError, Producer
import csv
from tabulate import tabulate
import psutil
import re
import sys

# Global variables for in-memory cache and aggregation
message_cache = []
cache_lock = threading.Lock()
aggregation_result = 0

# Initialize logging
log_location = 'logs/'  # Default log location
logging.basicConfig(filename=os.path.join(log_location, 'consumer.log'), level=logging.ERROR)

def process_message(message, select_query, data_format):
    # Process the message based on the data format
    if data_format == 'json':
        try:
            message_dict = json.loads(message)
            # Process the JSON message
            # Replace this with your actual JSON processing logic
            # Example: Extract a specific key from the JSON
            key = message_dict.get('key', '')
            result = f"Processed JSON message: {message_dict}, Select Query: {select_query}, Key: {key}"
        except json.JSONDecodeError:
            result = "Error: Invalid JSON message"
    elif data_format == 'avro':
        # Process the Avro message
        # Replace this with your actual Avro processing logic
        result = f"Processed Avro message: {message}, Select Query: {select_query}"
    else:
        # Assuming 'text' format for all other cases
        # Process the text message
        # Replace this with your actual text processing logic
        result = f"Processed text message: {message}, Select Query: {select_query}"

    return result

def run_select_query(select_query, data_format):
    global aggregation_result
    # Execute the select query, including aggregation if specified
    # Replace this with your actual query execution logic
    if data_format == 'json':
        # Example: Extract a specific key from JSON and perform aggregation
        key = re.search(r"'key' ?: ?'([^']+)'", select_query)
        if key:
            value = key.group(1)
            aggregation_result += int(value)  # Accumulate the result
            return f"Aggregated JSON messages: {aggregation_result}"
        else:
            return "Error: JSON key not found in query"
    elif data_format == 'avro':
        # Example: Perform Avro-specific processing and aggregation
        aggregation_result += 1  # Increment count
        return f"Aggregated Avro messages: {aggregation_result}"
    else:
        # Example: Perform text-specific processing and aggregation
        aggregation_result += 1  # Increment count
        return f"Aggregated text messages: {aggregation_result}"

def consume_messages(bootstrap_servers, topic, kafka_select_file, max_memory_percentage, max_threads, output_format,
                     output_to_module, module_name, module_location, druid_config, kafka_config, output_module_config,
                     datetime_format, prompt_terminal, stream_columns, stream_to_druid, stream_to_kafka):
    # Kafka consumer configuration
    consumer = Consumer(kafka_config['consumer_settings'])
    consumer.subscribe([topic])

    # Kafka producer configuration (if needed for streaming to Kafka)
    producer = None
    if stream_to_kafka:
        producer = Producer(kafka_config['producer_settings'])

    try:
        while True:
            msg = consumer.poll(1.0)

            if msg is None:
                continue

            if msg.error():
                if msg.error().code() == KafkaError._PARTITION_EOF:
                    logging.warning('Reached end of partition')
                else:
                    logging.error('Error: {}'.format(msg.error()))
            else:
                message = msg.value().decode('utf-8')
                logging.info(f"Received message: {message}")

                # Determine the select query based on kafka_select.json
                query_identifier = get_query_identifier(message)  # Implement logic to extract the query identifier
                select_query = get_select_query(kafka_select_file, query_identifier)

                # Process the message and store the result in-memory
                result = process_message(message, select_query, output_format)

                # Cache the result and update aggregation
                cache_result(result, max_memory_percentage)

                # Stream the output to various destinations based on configuration
                stream_output(result, stream_to_druid, stream_to_kafka, producer, output_module_config)

                # Pass the output to the specified module if enabled
                if output_to_module:
                    pass_output_to_module(result, module_name, module_location)

                # Print the output to terminal if prompt_terminal is enabled
                if prompt_terminal:
                    print(result)
                    sys.stdout.flush()

    except KeyboardInterrupt:
        pass
    finally:
        consumer.close()

def get_query_identifier(message):
    # Implement logic to extract the query identifier from the message content
    # For example, if the message contains a specific keyword, use it as the identifier
    if "error" in message:
        return "query1"
    elif "info" in message:
        return "query2"
    else:
        return "default"

def get_select_query(kafka_select_file, query_identifier):
    try:
        with open(kafka_select_file, 'r') as json_file:
            data = json.load(json_file)
            select_queries = data.get('select_queries', {})
            return select_queries.get(query_identifier, select_queries.get('default', ''))
    except FileNotFoundError:
        return ''

def cache_result(result, max_memory_percentage):
    global message_cache

    # Calculate the maximum cache size based on memory percentage
    max_cache_size = max_memory_percentage * 0.01 * psutil.virtual_memory().total

    with cache_lock:
        message_cache.append(result)

        # Check if cache size exceeds the maximum
        while len(message_cache) > max_cache_size:
            message_cache.pop(0)  # Remove the oldest entry

def stream_output(result, stream_to_druid, stream_to_kafka, producer, output_module_config):
    # Stream the output to various destinations based on configuration
    if stream_to_druid:
        stream_to_druid_datasource(result)
    if stream_to_kafka and producer:
        stream_to_kafka_topic(result, producer, output_module_config)
    # Add more streaming destinations as needed

def stream_to_druid_datasource(data):
    # Implement logic to stream data to a Druid datasource with dynamic headers
    # Use the headers as defined in the 'select_query' from kafka_select.json
    # You'll need to configure Druid endpoint and authentication here
    try:
        druid_url = druid_config.get('druid_url', '')
        headers = {'Content-Type': 'application/json'}
        response = requests.post(druid_url, headers=headers, json=data)

        if response.status_code == 200:
            logging.info("Data successfully streamed to Druid")
        else:
            logging.error(f"Error streaming data to Druid: {response.status_code}")
    except Exception as e:
        logging.error(f"Error streaming data to Druid: {e}")

def stream_to_kafka_topic(data, producer, output_module_config):
    # Implement logic to stream data to a Kafka topic with dynamic headers
    # Use the headers as defined in the 'select_query' from kafka_select.json
    # You'll need to configure Kafka producer settings for this
    try:
        topic = output_module_config.get('topic', 'your_kafka_topic')
        key = output_module_config.get('key', '')  # Define your key logic here
        producer.produce(topic, key=key, value=json.dumps(data))
        producer.flush()
        logging.info("Data successfully streamed to Kafka topic")
    except Exception as e:
        logging.error(f"Error streaming data to Kafka topic: {e}")

def pass_output_to_module(output, module_name, module_location):
    # Import and call the specified module with the output
    try:
        module = __import__(module_name)
        module.handle_output(output)
    except Exception as e:
        logging.error(f"Error executing the module: {e}")

def output_results(output_format, data):
    if output_format == 'csv':
        with open('output.csv', 'w', newline='') as csvfile:
            fieldnames = ["Message", "Select Query"]
            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)

            writer.writeheader()
            for row in data:
                writer.writerow({"Message": row, "Select Query": "your_select_query"})

    elif output_format == 'table':
        print(tabulate(data, headers=["Message", "Select Query"], tablefmt="grid"))

def process_messages_concurrently(data, max_threads):
    # Implement logic to process messages concurrently using CPU threads
    # You can use the threading module for this
    threads = []
    for _ in range(max_threads):
        thread = threading.Thread(target=process_message, args=(data,))
        thread.start()
        threads.append(thread)

    for thread in threads:
        thread.join()

def main():
    parser = argparse.ArgumentParser(description='Kafka Consumer')
    parser.add_argument('--topic', type=str, help='Input Kafka topic')
    parser.add_argument('--kafka_select', type=str, help='Path to kafka_select.json')
    parser.add_argument('--config', type=str, help='Path to config.json')
    args = parser.parse_args()

    config_path = args.config or 'config.json'

    process_config_file(config_path)

    # Load configuration parameters from config
    with open(config_path, 'r') as config_file:
        config = json.load(config_file)
        max_memory_percentage = config.get('max_memory_percentage', 20)
        max_threads = config.get('max_threads', 10)
        output_format = config.get('output_format', 'table')
        output_to_module = config.get('output_to_module', False)
        module_name = config.get('module_name', '')
        module_location = config.get('module_location', '')
        log_location = config.get('log_location', 'logs/')
        datetime_format = config.get('datetime_format', '%Y-%m-%d %H:%M:%S')
        prompt_terminal = config.get('continuous_output', True)
        stream_columns = config.get('stream_columns', False)
        druid_config = config.get('druid_config', {})
        kafka_config = config.get('kafka_config', {})
        output_module_config = config.get('output_module_config', {})

    # Create the log directory if it doesn't exist
    os.makedirs(log_location, exist_ok=True)

    # Initialize logging with the specified log location
    logging.basicConfig(filename=os.path.join(log_location, 'consumer.log'), level=logging.ERROR)

    # Determine the streaming destination based on command-line arguments
    stream_to_druid = args.druid
    stream_to_kafka = args.kafka

    # Start multiple consumer threads
    consumer_threads = []
    for _ in range(max_threads):
        consumer_thread = threading.Thread(target=consume_messages, args=(
            kafka_config.get('bootstrap.servers', ''), args.topic, args.kafka_select, max_memory_percentage,
            max_threads, output_format, output_to_module, module_name, module_location, druid_config, kafka_config,
            output_module_config, datetime_format, prompt_terminal, stream_columns, stream_to_druid, stream_to_kafka))
        consumer_thread.start()
        consumer_threads.append(consumer_thread)

    # Create a temporary file for caching
    with tempfile.TemporaryDirectory() as tmpdirname:
        cache_file_path = os.path.join(tmpdirname, 'cache.txt')

        while True:
            with cache_lock:
                if message_cache:
                    with open(cache_file_path, 'a') as cache_file:
                        for item in message_cache:
                            cache_file.write(json.dumps(item) + '\n')
                    message_cache.clear()

            if output_format == 'csv' or output_format == 'table':
                output_results(output_format, message_cache)
                message_cache.clear()

if __name__ == '__main__':
    main()
